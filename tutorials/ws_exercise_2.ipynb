{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and define all the necessary modules and models for this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure, the numpy library only uses one thread\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"]=\"1\"\n",
    "\n",
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.interpolate\n",
    "\n",
    "# import models\n",
    "from earm.lopez_direct import model as md\n",
    "from earm.lopez_indirect import model as mi\n",
    "from earm.lopez_embedded import model as me\n",
    "from pysb.simulator import ScipyOdeSimulator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "load the experimental data, read from the data file the time points at which the data was taken - these will be used as integration points for the models themselves;\n",
    "\n",
    "a set of parameter values for the model is hardcoded in the model itself - extract these parameter values for each model \n",
    "\n",
    "    param_values_d = np.array([p.value for p in md.parameters])\n",
    "    \n",
    "since we do not want to specify the arguments for the type of simulation for all three models separately, they are getting stored in the \"args\" variable \n",
    "\n",
    "perform a simulation for all three models using the time points from the data file\n",
    "\n",
    "    solver_d = ScipyOdeSimulator(md, tspan, **args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experimental data\n",
    "exp_data = pd.read_csv('EC-RP_IMS-RP_IC-RP_data_for_models.csv', index_col=False)\n",
    "\n",
    "# read time points from data file\n",
    "tspan = exp_data['# Time'].values.copy()\n",
    "\n",
    "# use the rate parameters that are hardcoded into the model\n",
    "param_values_i = np.array([p.value for p in mi.parameters])\n",
    "# TODO\n",
    "# TODO\n",
    "\n",
    "# arguments for the model simulation (solve the ODEs)\n",
    "args = {'integrator': 'lsoda', 'use_analytic_jacobian': True}\n",
    "\n",
    "# simulate the models\n",
    "solver_i = ScipyOdeSimulator(mi, tspan, **args)\n",
    "# TODO\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "extract the solution trajectories from the simulation; \n",
    "normalize the solution to the first parameter value (the normalizing factor is the same for all three different models); \n",
    "the data file measures three parts in the model: Bid, Smac, PARP -> we want to analyse how the model reproduces those parameters; mBid, aSmac, PARP are observables provided by the models\n",
    "\n",
    "        traj_d = solver_d.run(param_values=param_values_d)\n",
    "        \n",
    "        bid_traj_d = traj_d.observables['mBid'] / normal_bid_factor\n",
    "        \n",
    "        aSmac_traj_d = traj_d.observables['aSmac'] / normal_smac_factor\n",
    "        \n",
    "        cparp_traj_d = traj_d.observables['cPARP'] / normal_parp_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trajectories of the ODE integration for analysis\n",
    "traj_i = solver_i.run(param_values=param_values_i)\n",
    "# TODO \n",
    "# TODO\n",
    "\n",
    "# normalize the trajectories to the initial conditions\n",
    "normal_bid_factor = mi.parameters['Bid_0'].value\n",
    "normal_smac_factor = mi.parameters['Smac_0'].value\n",
    "normal_parp_factor = mi.parameters['PARP_0'].value\n",
    "\n",
    "bid_traj_i = traj_i.observables['mBid'] / normal_bid_factor\n",
    "# TODO\n",
    "# TODO\n",
    "\n",
    "aSmac_traj_i = traj_i.observables['aSmac'] / normal_smac_factor\n",
    "# TODO\n",
    "# TODO\n",
    "\n",
    "cparp_traj_i = traj_i.observables['cPARP'] / normal_parp_factor\n",
    "# TODO\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plotting parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and compare tBid\n",
    "\n",
    "plt.plot(tspan, bid_traj_i, color='r', marker='^', label='tBID, indirect model')\n",
    "# TODO\n",
    "# TODO\n",
    "plt.errorbar(exp_data['# Time'], exp_data['norm_IC-RP'],yerr=exp_data['nrm_var_IC-RP'] ** .5,\n",
    "             ecolor='black', color='black', elinewidth=0.5, capsize=0)\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and compare Smac\n",
    "# the release of Smac is in a Snap action ([Albeck2008])\n",
    "# this data displays the time in which this snap happens\n",
    "\n",
    "# Mean and variance of Td (delay time) and Ts (switching time) of MOMP, and\n",
    "# yfinal (the last value of the IMS-RP trajectory)\n",
    "momp_data = np.array([9810.0, 180.0, mi.parameters['Smac_0'].value])\n",
    "momp_var = np.array([7245000.0, 3600.0, 1e4])\n",
    "\n",
    "plt.plot(tspan, aSmac_traj_i, color='r', label='aSMAC, indirect model')\n",
    "# TODO\n",
    "# TODO\n",
    "plt.axvline(momp_data[0], -0.05, 1.05, color='black', linestyle=':',label='exp aSMAC')\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and compare PARP\n",
    "\n",
    "plt.plot(tspan, cparp_traj_i, color='r', marker='*', label='cPARP, indirect model')\n",
    "# TODO\n",
    "# TODO\n",
    "plt.errorbar(exp_data['# Time'], exp_data['norm_EC-RP'],\n",
    "                 yerr=exp_data['nrm_var_EC-RP'] ** .5,\n",
    "                 ecolor='black', color='black', elinewidth=0.5, capsize=0)\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "IF YOU HAVE ALL THREE PLOTS, PLEASE TELL US AND WE WILL TALK ABOUT THE NEXT STEPS WITH THE GROUP!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PART II "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "here is a function defined for the likelihood that we will use in the next steps - feel free to take a look at it how it works, but don't worry about it if you don't want to go into the details right now - just execute this part so that it can get used by the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function which evaluates how far the model is away from the real data\n",
    "# this cost function is based on the chi^2 test\n",
    "# this cost function is specifically tailored to EARM\n",
    "\n",
    "def likelihood(position):\n",
    "    param_values[rate_mask] = 10 ** position\n",
    "    traj = solver.run(param_values=param_values)\n",
    "\n",
    "    model = mi\n",
    "    # normalize trajectories\n",
    "    bid_traj = traj.observables['mBid'] / model.parameters['Bid_0'].value\n",
    "    cparp_traj = traj.observables['cPARP'] / model.parameters['PARP_0'].value\n",
    "    momp_traj = traj.observables['aSmac']\n",
    "\n",
    "    # calculate chi^2 distance for each time course\n",
    "    e1 = np.sum((exp_data['norm_IC-RP'] - bid_traj) ** 2 /\n",
    "                (2 * exp_data['nrm_var_IC-RP'])) / len(bid_traj)\n",
    "\n",
    "    e2 = np.sum((exp_data['norm_EC-RP'] - cparp_traj) ** 2 /\n",
    "                (2 * exp_data['nrm_var_EC-RP'])) / len(cparp_traj)\n",
    "\n",
    "    # Here we fit a spline to find where we get 50% release of MOMP reporter\n",
    "    if np.nanmax(momp_traj) == 0:\n",
    "        print('No aSmac!')\n",
    "        t10 = 0\n",
    "        t90 = 0\n",
    "    else:\n",
    "        ysim_momp_norm = momp_traj / np.nanmax(momp_traj)\n",
    "        st, sc, sk = scipy.interpolate.splrep(tspan, ysim_momp_norm)\n",
    "        try:\n",
    "            t10 = scipy.interpolate.sproot((st, sc - 0.10, sk))[0]\n",
    "            t90 = scipy.interpolate.sproot((st, sc - 0.90, sk))[0]\n",
    "        except IndexError:\n",
    "            t10 = 0\n",
    "            t90 = 0\n",
    "\n",
    "    # time of death  = halfway point between 10 and 90%\n",
    "    td = (t10 + t90) / 2\n",
    "\n",
    "    # time of switch is time between 90 and 10 %\n",
    "    ts = t90 - t10\n",
    "\n",
    "    # final fraction of aSMAC (last value)\n",
    "    yfinal = momp_traj[-1]\n",
    "    momp_sim = [td, ts, yfinal]\n",
    "\n",
    "    e3 = np.sum((momp_data - momp_sim) ** 2 / (2 * momp_var)) / 3\n",
    "    # return sum of errors ( the ',' is required)\n",
    "    return e1 + e2 + e3,"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "to actually train the rates of the model to the data, we use Particle Swarm Optimization - this is provided by another package, that we now need to load;\n",
    "\n",
    "the paramters from the models actually also include initial conditions - however, we only want to train the rates of the models to fit the data points with the PSO -> the rate mask loops over the model parameters and only extracts the rate parameters (the initial conditions are therefore filtered out);\n",
    "\n",
    "starting positions are needed to give an initial condition for the PSO - we start with the original parameter values from the models - note, however, that we transform them into log space - this is due to the fact that it does not make sense to search for parameters linearly (for a rate, it does not matter, if it is 10 or 11, but we are actually interested in orders of magnitude, i.e.: 10 or 100) - to account for the logarithmic search engine of PSO, the initial parameters are transformed into log space too;\n",
    "\n",
    "run the PSO for the first model (this will take a while and will give you results that might be interesting - we will talk about those after the exercise): for running a PSO, we need the rate constants as new parameter values, the correct rate mask, the correct solver (i.e. here is the model chosen), a cost function (which we defined above), the starting position and bounds so that the PSO does not make steps outside of the actual domain\n",
    "with all those informations, we can now .run the PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load simple PSO for the particle swarm optimization\n",
    "# this method re-evaluates the rate constants\n",
    "# and tailors them to the real data\n",
    "from simplepso.pso import PSO\n",
    "\n",
    "# get the original parameters from the model\n",
    "rate_params_i = mi.parameters_rules()\n",
    "#TODO\n",
    "#TODO\n",
    "\n",
    "# the parameter set also includes initial conditions \n",
    "# those are not to be changed, so this mask filters\n",
    "# purely the rates from the parameters which need to be trained\n",
    "rate_mask_i = np.array([p in rate_params_i for p in mi.parameters])\n",
    "#TODO\n",
    "#TODO\n",
    "\n",
    "# the original values are used as starting position for the PSO\n",
    "# PSO searches in log-space for efficiency, therefore, the\n",
    "# initial data is transformed into log-space\n",
    "starting_position_i = np.log10(param_values_i[rate_mask_i])\n",
    "#TODO\n",
    "#TODO\n",
    "\n",
    "# create a PSO object for the first model\n",
    "# the parameter values to be trained are the rates from this model\n",
    "param_values = param_values_i\n",
    "rate_mask = rate_mask_i\n",
    "solver = solver_i\n",
    "pso_i = PSO(save_sampled=False, verbose=True, num_proc=4)\n",
    "# use the above defined likelihood function as cost-function for the PSO\n",
    "pso_i.set_cost_function(likelihood)\n",
    "pso_i.set_start_position(starting_position_i)\n",
    "pso_i.set_bounds(2)\n",
    "\n",
    "# run to minimize for the best fit \n",
    "# if stopping criteria is reached, run again\n",
    "pso_i.run(num_particles=25, num_iterations=50, stop_threshold=1e-5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "this run gave you new rate parameters - if you want to see the values of these, you can display them via the display() command\n",
    "\n",
    "due to the log-space search of the PSO, we need to transform the new rates back into linear space, then we save those parameters into a file so we do not loose them\n",
    "\n",
    "run the indirect model again, but now use the new rate parameters - in the end, again save the resulting trajectories in variables so that we can plot them later on again, the rate_mask argument makes sure that the rates are getting stored into the correct space and do not accidentally override some initial conditions\n",
    "\n",
    "(in this box, there is actually no active exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new rate parameters for the indirect model\n",
    "display(pso_i.best)\n",
    "\n",
    "# run indirect model with newly suggested rates\n",
    "# results of PSO are in log-space -> retransform into normal space\n",
    "param_values[rate_mask] = 10 ** pso_i.best\n",
    "\n",
    "# save the new parameters for later use in an external csv file\n",
    "np.savetxt(\"fitted_model_i_parameters.csv\", param_values)\n",
    "\n",
    "# run the indirect model with the new rate parameters\n",
    "traj_i = solver_i.run(param_values=param_values)\n",
    "\n",
    "# normalize the results again\n",
    "bid_traj_i = traj_i.observables['mBid'] / normal_bid_factor\n",
    "cparp_traj_i = traj_i.observables['cPARP'] / normal_parp_factor\n",
    "aSmac_traj_i = traj_i.observables['aSmac'] / normal_smac_factor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "now do the same for the direct model (all steps of the PSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do PSO for the direct model\n",
    "\n",
    "param_values = #TODO\n",
    "rate_mask = #TODO\n",
    "solver = #TODO\n",
    "pso_d = PSO(save_sampled=False, verbose=True, num_proc=4)\n",
    "pso_d.set_cost_function(likelihood)\n",
    "pso_d.set_start_position(#TODO)\n",
    "pso_d.set_bounds(2)\n",
    "\n",
    "pso_d.run(num_particles=25, num_iterations=50, stop_threshold=1e-5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "again run the direct model with the new rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run direct model with newly suggested rates\n",
    "param_values[rate_mask] = 10 ** pso_d.best\n",
    "np.savetxt(\"fitted_model_d_parameters.csv\", param_values)\n",
    "traj_d = #TODO\n",
    "\n",
    "bid_traj_d = traj_d.observables['mBid'] / normal_bid_factor\n",
    "cparp_traj_d = #TODO\n",
    "aSmac_traj_d = #TODO"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "now do the PSO for the embedded model on your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do PSO for the embedded model\n",
    "\n",
    "#TODO ALL THE STEPS OF PSO FOR THE EMBEDDED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run embedded model with newly suggested rates\n",
    "\n",
    "#TODO ALL THE STEPS TO RE-RUN THE EMBEDDED MODEL WITH THE NEW RATES"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "now, let's see how our three models are actually performing by comparing them with the data points again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the models using the new rates with the experimental data\n",
    "\n",
    "plt.plot(tspan, bid_traj_i, color='r', marker='^', label='tBID, indirect model')\n",
    "#TODO\n",
    "#TODO\n",
    "plt.errorbar(exp_data['# Time'], exp_data['norm_IC-RP'],\n",
    "                 yerr=exp_data['nrm_var_IC-RP'] ** .5,\n",
    "                 ecolor='black', color='black', elinewidth=0.5, capsize=0)\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tspan, aSmac_traj_i, color='r', label='aSMAC, indirect model')\n",
    "#TODO\n",
    "#TODO\n",
    "plt.axvline(momp_data[0], -0.05, 1.05, color='black', linestyle=':',\n",
    "               label='exp aSMAC')\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tspan, cparp_traj_i, color='r', label='cPARP, indirect model')\n",
    "#TODO\n",
    "#TODO\n",
    "plt.errorbar(exp_data['# Time'], exp_data['norm_EC-RP'],\n",
    "                 yerr=exp_data['nrm_var_EC-RP'] ** .5,\n",
    "                 ecolor='black', color='black', elinewidth=0.5, capsize=0)\n",
    "plt.legend(loc=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
